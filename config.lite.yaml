# pllm LITE Mode Configuration
# This configuration is for running pllm without external dependencies
# Only core LLM proxy and routing features are available

# Server configuration
server:
  port: 8080
  host: "0.0.0.0"
  read_timeout: 60s
  write_timeout: 60s
  idle_timeout: 120s
  graceful_shutdown: 30s
  
# Database configuration (disabled in lite mode)
database:
  url: ""  # Empty to disable
  
# Redis configuration (disabled in lite mode) 
redis:
  url: ""  # Empty to disable
  
# Cache configuration (in-memory only in lite mode)
cache:
  enabled: false  # Disable persistent cache
  
# Rate limiting configuration (in-memory only in lite mode)
rate_limit:
  enabled: true  # In-memory rate limiting in lite mode
  global_rpm: 60  # Global requests per minute
  chat_completions_rpm: 30  # Chat completions per minute
  completions_rpm: 30  # Completions per minute
  embeddings_rpm: 100  # Embeddings per minute
  requests_per_minute: 60  # Default fallback
  burst: 10  # Allow burst of 10 requests
  cleanup_interval: 5m  # Cleanup interval for expired entries
  
# Authentication configuration (disabled in lite mode)
auth:
  enabled: false
  
# Logging configuration
logging:
  level: info
  format: json
  
# Router configuration
router:
  routing_strategy: "priority"  # priority, round-robin, weighted, least-busy, latency-based, usage-based
  fallback_enabled: true
  retry_attempts: 2
  timeout: 30s
  health_check_interval: 30s

# Model-centric configuration
# Configure your LLM instances here
model_list:
  # Example: GPT-3.5 Turbo with your API key
  - id: gpt-3.5-turbo-default
    model_name: gpt-3.5-turbo
    instance_name: default
    enabled: true
    priority: 100
    weight: 1.0
    rpm: 200  # requests per minute
    tpm: 90000  # tokens per minute
    provider:
      type: openai
      model: gpt-3.5-turbo
      api_key: ${OPENAI_API_KEY}  # Set via environment variable
      base_url: https://api.openai.com/v1
    model_info:
      mode: chat
      supports_functions: true
      supports_vision: false
      supports_streaming: true
      max_tokens: 16385
      max_input_tokens: 16385
      max_output_tokens: 4096
      default_max_tokens: 1000
    tags:
      - lite-mode
      - default

  # Add more model instances as needed
  # Example with multiple API keys for load balancing:
  
  # - id: gpt-3.5-turbo-backup
  #   model_name: gpt-3.5-turbo
  #   instance_name: backup
  #   enabled: true
  #   priority: 90
  #   weight: 1.0
  #   rpm: 200
  #   tpm: 90000
  #   provider:
  #     type: openai
  #     model: gpt-3.5-turbo
  #     api_key: ${OPENAI_API_KEY_2}
  #     base_url: https://api.openai.com/v1
  #   model_info:
  #     mode: chat
  #     supports_functions: true
  #     supports_vision: false
  #     supports_streaming: true
  #     max_tokens: 16385
  #     max_input_tokens: 16385
  #     max_output_tokens: 4096
  #     default_max_tokens: 1000
  #   tags:
  #     - lite-mode
  #     - backup

  # Example: GPT-4 (uncomment to use)
  # - id: gpt-4-default
  #   model_name: gpt-4
  #   instance_name: default
  #   enabled: true
  #   priority: 100
  #   weight: 1.0
  #   rpm: 60
  #   tpm: 90000
  #   provider:
  #     type: openai
  #     model: gpt-4
  #     api_key: ${OPENAI_API_KEY}
  #     base_url: https://api.openai.com/v1
  #   model_info:
  #     mode: chat
  #     supports_functions: true
  #     supports_vision: false
  #     supports_streaming: true
  #     max_tokens: 8192
  #     max_input_tokens: 8192
  #     max_output_tokens: 4096
  #     default_max_tokens: 1000
  #   tags:
  #     - lite-mode
  #     - premium

  # Example: Anthropic Claude (uncomment to use)
  # - id: claude-3-sonnet
  #   model_name: claude-3-sonnet-20240229
  #   instance_name: default
  #   enabled: true
  #   priority: 100
  #   weight: 1.0
  #   rpm: 100
  #   tpm: 100000
  #   provider:
  #     type: anthropic
  #     model: claude-3-sonnet-20240229
  #     api_key: ${ANTHROPIC_API_KEY}
  #     base_url: https://api.anthropic.com
  #   model_info:
  #     mode: chat
  #     supports_functions: false
  #     supports_vision: true
  #     supports_streaming: true
  #     max_tokens: 200000
  #     max_input_tokens: 200000
  #     max_output_tokens: 4096
  #     default_max_tokens: 1000
  #   tags:
  #     - lite-mode
  #     - anthropic