# PLLM Configuration

server:
  port: 8080

database:
  url: postgres://pllm:pllm@localhost:5432/pllm?sslmode=disable

redis:
  url: redis://localhost:6379

jwt:
  secret_key: your-super-secret-jwt-key-change-this

admin:
  username: admin
  password: changeme123!
  email: admin@pllm.io

# Router configuration
router:
  routing_strategy: "latency-based"
  circuit_breaker_enabled: true
  
  # Fallback mapping: model_name -> [fallback_model_names]
  fallbacks:
    my-gpt-4: ["my-gpt-35-turbo", "my-gpt-35-turbo-16k"]
    my-gpt-35-turbo: ["my-gpt-35-turbo-16k"]
  
  # Context window fallbacks (when request is too large)
  context_window_fallbacks:
    my-gpt-35-turbo: ["my-gpt-35-turbo-16k"]
    my-gpt-4: ["my-gpt-4-32k"]

# Model list - User calls these names in API requests
model_list:
  # GPT-4 deployment
  - model_name: my-gpt-4  # What users call in API requests
    params:
      model: gpt-4  # Actual OpenAI model name
      api_key: ${OPENAI_API_KEY}
    
  # GPT-3.5-Turbo deployment  
  - model_name: my-gpt-35-turbo
    params:
      model: gpt-3.5-turbo
      api_key: ${OPENAI_API_KEY}
    
  # GPT-3.5-Turbo-16K deployment
  - model_name: my-gpt-35-turbo-16k
    params:
      model: gpt-3.5-turbo-16k
      api_key: ${OPENAI_API_KEY}
    
  # Example with optional fields
  - model_name: fast-gpt
    params:
      model: gpt-3.5-turbo
      api_key: ${OPENAI_API_KEY}
      temperature: 0.7
      max_tokens: 2000
    rpm: 500  # Optional rate limits
    tpm: 100000
    
  # Example with Azure
  - model_name: azure-gpt-4
    params:
      model: azure/my-deployment-name
      api_base: https://my-azure-endpoint.openai.azure.com/
      api_key: ${AZURE_API_KEY}
      api_version: 2024-02-15-preview

# Model aliases/groups (optional)
model_aliases:
  smart: ["my-gpt-4", "azure-gpt-4"]
  fast: ["my-gpt-35-turbo", "fast-gpt"]
  long-context: ["my-gpt-35-turbo-16k"]