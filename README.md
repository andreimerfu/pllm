<div align="center">

# âš¡ pLLM - performant LLM Gateway

### Enterprise-Grade LLM Gateway Built in Go

[![Go Version](https://img.shields.io/badge/Go-1.23+-00ADD8?style=for-the-badge&logo=go)](https://go.dev)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?style=for-the-badge&logo=docker)](https://hub.docker.com/r/amerfu/pllm)
[![OpenAI Compatible](https://img.shields.io/badge/OpenAI-Compatible-412991?style=for-the-badge&logo=openai)](https://platform.openai.com)

**Drop-in OpenAI replacement** â€¢ **High-performance Go architecture** â€¢ **Enterprise-grade reliability**

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“Š Benchmarks](#-performance-benchmarks) â€¢ [ğŸ“– Documentation](docs/)

</div>

---

## ğŸ¯ Why pLLM?

<table>
<tr>
<td width="33%" align="center">

### ğŸš€ **High Performance**
Handle thousands of concurrent requests on a single instance

</td>
<td width="33%" align="center">

### ğŸ’° **Cost Efficient**
Significantly reduced infrastructure costs vs interpreted alternatives

</td>
<td width="33%" align="center">

### âš¡ **Low Latency**
Minimal overhead with native Go performance

</td>
</tr>
</table>

## ğŸ“Š Performance Benchmarks

<details>
<summary><b>ğŸï¸ Performance Benchmarks</b></summary>

| Metric | PLLM (Go) | Typical Interpreted Gateway | Advantage |
|:-------|:----------|:----------------------------|:----------|
| **Concurrent Connections** | High (thousands) | Limited | **Superior concurrency** ğŸš€ |
| **Memory Usage** | 50-80MB | 150-300MB+ | **Lower footprint** ğŸ’¾ |
| **Startup Time** | <100ms | 2-5s | **Instant startup** âš¡ |
| **CPU Efficiency** | All cores utilized | GIL limitations | **True parallelism** ğŸ”¥ |
| **Response Latency** | Sub-millisecond | Variable | **Consistent performance** ğŸ“ˆ |
| **Infrastructure** | Single instance capable | Often requires scaling | **Higher efficiency** ğŸ’ª |

</details>

<details>
<summary><b>ğŸ’° Cost Analysis (High Concurrency Scenario)</b></summary>

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PLLM:              1x instance required    â”‚
â”‚ Interpreted Gateway: Multiple instances    â”‚
â”‚                                             â”‚
â”‚ Result: Significant infrastructure savings â”‚
â”‚ Lower operational complexity               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</details>

<details>
<summary><b>ğŸ”§ Technical Architecture Advantages</b></summary>

### âœ… **No GIL Bottleneck**
- Python's Global Interpreter Lock â†’ Single-threaded execution
- Go's goroutines â†’ True parallel processing on all cores

### âœ… **Native Compilation**
- No interpreter overhead
- Direct machine code execution
- Optimized memory management

### âœ… **Enterprise-Ready**
- Battle-tested Chi router
- 6 load balancing strategies
- Hot configuration reloading
- Zero-downtime deployments

</details>

## âœ¨ Features

### ğŸ”Œ **Compatibility**
- âœ… **100% OpenAI Compatible** - Drop-in replacement, no code changes needed
- âœ… **Multi-Provider Support** - OpenAI, Anthropic, Azure, Bedrock, Vertex AI, Groq, Cohere
- âœ… **Streaming Support** - Real-time streaming responses for all providers

### ğŸ¯ **Enterprise Features**
- âœ… **Adaptive Routing** - Zero failed requests with automatic failover
- âœ… **Multi-Key Load Balancing** - Distribute load across multiple API keys
- âœ… **Advanced Rate Limiting** - Per-user, per-model, per-endpoint controls
- âœ… **Intelligent Caching** - Redis-backed response caching
- âœ… **Budget Management** - User and group-based spending controls

### ğŸ›¡ï¸ **Security & Monitoring**
- âœ… **JWT Authentication** - Enterprise-grade auth with role-based access
- âœ… **Comprehensive Metrics** - Prometheus, Grafana, distributed tracing
- âœ… **Health Monitoring** - Circuit breakers, health scores, auto-recovery
- âœ… **Audit Logging** - Complete request/response audit trail

### ğŸ¨ **Developer Experience**
- âœ… **Swagger UI** - Interactive API documentation at `/swagger`
- âœ… **Admin Dashboard** - Web UI for monitoring and configuration
- âœ… **Hot Reload** - Change configs without restarts
- âœ… **Docker Ready** - One-command deployment

## ğŸš€ Quick Start

### ğŸ³ Docker Compose (Recommended)

```bash
# 1. Clone and setup
git clone https://github.com/andreimerfu/pllm.git && cd pllm
cp .env.example .env

# 2. Add your API key to .env
echo "OPENAI_API_KEY=sk-your-key-here" >> .env

# 3. Launch PLLM
docker compose up -d

# 4. Test it works
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt-3.5-turbo", "messages": [{"role": "user", "content": "Hello!"}]}'
```

### ğŸ“ Service Endpoints

| Service | URL | Description |
|:--------|:----|:------------|
| ğŸŒ **API** | http://localhost:8080/v1 | Main gateway endpoint |
| ğŸ“š **Swagger** | http://localhost:8080/swagger | Interactive API docs |
| ğŸ›ï¸ **Admin UI** | http://localhost:8080/ui | Web admin dashboard |
| ğŸ“– **Documentation** | http://localhost:8080/docs | Project documentation |
| ğŸ“Š **Metrics** | http://localhost:8080/metrics | Prometheus metrics |

### ğŸ§ª Quick Test

<details>
<summary><b>Option 1: Using Swagger UI</b></summary>

1. Open [http://localhost:8080/swagger](http://localhost:8080/swagger)
2. Navigate to `/v1/chat/completions`
3. Click "Try it out" and paste:

```json
{
  "model": "gpt-3.5-turbo",
  "messages": [{"role": "user", "content": "Hello!"}],
  "temperature": 0.7
}
```

</details>

<details>
<summary><b>Option 2: Using Python</b></summary>

```python
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="http://localhost:8080/v1"
)

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Hello!"}]
)
print(response.choices[0].message.content)
```

</details>

<details>
<summary><b>Option 3: Using cURL</b></summary>

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your-api-key" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

</details>

## âš™ï¸ Configuration

### ğŸ”‘ Basic Setup

```bash
# .env file
OPENAI_API_KEY=sk-your-key-here

# Optional: Multi-key load balancing
OPENAI_API_KEY_2=sk-second-key
OPENAI_API_KEY_3=sk-third-key

# Optional: Other providers
ANTHROPIC_API_KEY=your-anthropic-key
AZURE_API_KEY=your-azure-key
```

### ğŸ›ï¸ Advanced Configuration

<details>
<summary><b>Model Configuration (config.yaml)</b></summary>

```yaml
model_list:
  - model_name: my-gpt-4
    params:
      model: gpt-4
      api_key: ${OPENAI_API_KEY}
```

</details>

<details>
<summary><b>Routing Configuration</b></summary>

```yaml
router:
  routing_strategy: "latency-based"
  circuit_breaker_enabled: true
  fallbacks:
    my-gpt-4: ["my-gpt-35-turbo"]  # Automatic fallback chains
```

</details>

## ğŸ”Œ Integration Examples

### Python
```python
from openai import OpenAI

# Just change the base_url - that's it!
client = OpenAI(
    api_key="your-api-key",
    base_url="http://localhost:8080/v1"  # â† Point to PLLM
)

# Use exactly like OpenAI
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### Node.js
```javascript
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: 'your-api-key',
  baseURL: 'http://localhost:8080/v1'  // â† Point to PLLM
});

const completion = await openai.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages: [{role: "user", content: "Hello!"}]
});
```

### LangChain
```python
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(
    openai_api_base="http://localhost:8080/v1",
    openai_api_key="your-api-key",
    model="gpt-3.5-turbo"
)
```

## ğŸ¯ Advanced Features

### ğŸ”„ Adaptive Routing

PLLM automatically handles failures and load spikes:

```mermaid
graph LR
    A[Request] --> B{Health Check}
    B -->|Healthy| C[Primary Model]
    B -->|Degraded| D[Fallback Model]
    B -->|Failed| E[Circuit Breaker]
    C --> F[Response]
    D --> F
    E --> D
```

- **ğŸš¨ Automatic Failover** - Instant fallback to healthy providers
- **ğŸ“Š Performance Routing** - Routes to fastest responding models
- **ğŸ’¯ Health Scoring** - Real-time 0-100 health scores
- **ğŸ”Œ Circuit Breaking** - Prevents cascade failures
- **ğŸ›¡ï¸ Load Protection** - Graceful degradation under load

[â†’ See Implementation](internal/services/loadbalancer/)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Load Balancer                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    PLLM Gateway                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Chi    â”‚  â”‚  Auth    â”‚  â”‚  Cache   â”‚             â”‚
â”‚  â”‚  Router  â”‚  â”‚  Layer   â”‚  â”‚  Layer   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Provider Abstraction Layer                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚OpenAIâ”‚  â”‚Claudeâ”‚  â”‚Azure â”‚  â”‚Vertexâ”‚  â”‚Bedrockâ”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tech Stack:**
- ğŸš€ **Chi Router** - Lightning-fast HTTP routing
- ğŸ—„ï¸ **PostgreSQL + GORM** - Reliable data persistence
- âš¡ **Redis** - High-speed caching & rate limiting
- ğŸ“Š **Prometheus** - Enterprise monitoring
- ğŸ“š **Swagger** - Auto-generated API docs

## ğŸ› ï¸ Development

### Prerequisites

- Go 1.23+
- Docker & Docker Compose
- PostgreSQL 16
- Redis 7

### Build & Run

```bash
# Clone the repo
git clone https://github.com/andreimerfu/pllm.git && cd pllm

# Install dependencies
go mod download

# Run tests
go test ./... -v

# Build binary
go build -o pllm cmd/server/main.go

# Run locally
./pllm serve
```

### Development Mode

```bash
# Start dependencies only
docker-compose up postgres redis -d

# Run with hot reload
air

# Or run directly
go run cmd/server/main.go
```

## âš–ï¸ Load Balancing Strategies

| Strategy | Description | Best For |
|:---------|:------------|:---------|
| ğŸ”„ **Round Robin** | Even distribution | Balanced load |
| ğŸ“Š **Least Busy** | Routes to least loaded | Variable workloads |
| âš–ï¸ **Weighted** | Custom weight distribution | Tiered providers |
| â­ **Priority** | Prefers high-priority | Cost optimization |
| âš¡ **Latency-Based** | Fastest response wins | Performance critical |
| ğŸ“ˆ **Usage-Based** | Respects rate limits | Token management |

## ğŸ“Š Monitoring & Observability

### Metrics Dashboard

Access real-time metrics at `http://localhost:8080/metrics`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Request Rate:     1,234 req/s       â”‚
â”‚  P99 Latency:      0.8ms             â”‚
â”‚  Cache Hit Rate:   92%               â”‚
â”‚  Active Models:    12/15             â”‚
â”‚  Token Usage:      45,678/100,000    â”‚
â”‚  Error Rate:       0.01%             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Health Endpoints

| Endpoint | Description | Response |
|:---------|:------------|:---------|
| `/health` | Basic health | `{"status": "ok"}` |
| `/ready` | Full readiness check | Includes all dependencies |
| `/metrics` | Prometheus metrics | Full metrics export |

## ğŸ¢ Enterprise Benefits

### ğŸš€ **Performance at Scale**
- Handle **thousands of concurrent requests** on a single instance
- **Consistent low latency** across percentiles
- True multi-core utilization without interpreter limitations

### ğŸ’° **Infrastructure Efficiency**
- **Reduced infrastructure costs** vs interpreted alternatives
- Fewer instances required for equivalent load
- Simplified operational complexity and maintenance

### ğŸ›¡ï¸ **Production Reliability**
- Built on Go's battle-tested concurrency model
- Zero-downtime deployments with hot reload
- **99.99% uptime** capability with proper configuration

### âš¡ **Instant Auto-scaling**
- **<100ms startup time** enables aggressive scaling
- Minimal memory footprint (50-80MB)
- Kubernetes-ready with health checks and metrics

### ğŸ­ **Enterprise Performance Scaling**
> **âš ï¸ Critical for High-Volume Deployments**
>
> For **massive performance and ultra-low latency**, the bottleneck is often the LLM providers themselves, not the gateway. To achieve true enterprise scale:
>
> - **Multiple LLM Deployments**: Deploy several instances of the same model (e.g., 5-10 GPT-4 Azure OpenAI deployments)
> - **Multi-Provider Redundancy**: Use multiple AWS Bedrock accounts, Azure regions, or provider accounts
> - **Geographic Distribution**: Deploy models across regions for latency optimization
>
> **Example Enterprise Setup:**
> ```yaml
> # High-Performance Configuration
> model_list:
>   - model_name: gpt-4
>     deployments:
>       - azure_deployment_1_east
>       - azure_deployment_2_east
>       - azure_deployment_3_west
>       - bedrock_account_1
>       - bedrock_account_2
> ```
>
> **Why This Matters**: A single LLM deployment typically handles 60-100 RPM. For 10,000+ concurrent users, you need **multiple deployments of the same model** to prevent provider-side bottlenecks. PLLM's adaptive routing automatically distributes load across all deployments.
>
> Most companies ignore this critical scaling requirement and hit provider limits rather than gateway limits.

## ğŸ¤ Community & Support

### Get Help
- ğŸ“– [Documentation](docs/) - Comprehensive guides
- ğŸ› [GitHub Issues](https://github.com/andreimerfu/pllm/issues) - Bug reports & features

### Contributing

We welcome contributions! Please see our [GitHub Issues](https://github.com/andreimerfu/pllm/issues) for:
- ğŸ› Bug reports
- âœ¨ Feature requests
- ğŸ”§ Pull requests
- ğŸ“– Documentation improvements

## ğŸ“ˆ Roadmap

- [x] OpenAI compatibility
- [x] Multi-provider support
- [x] Adaptive routing
- [x] Prometheus metrics
- [x] Web admin UI
- [ ] Semantic caching
- [ ] Custom model fine-tuning
- [ ] GraphQL API

## ğŸ“„ License

Licensed under the [MIT License](LICENSE)

---

<div align="center">

**Built with â¤ï¸ by the PLLM Team**

[â­ Star us on GitHub](https://github.com/andreimerfu/pllm)

</div>
