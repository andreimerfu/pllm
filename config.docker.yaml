# PLLM Docker Configuration

server:
  port: 8080

database:
  url: postgres://pllm:pllm@postgres:5432/pllm?sslmode=disable

redis:
  url: redis://redis:6379

jwt:
  secret_key: your-super-secret-jwt-key-change-this

admin:
  username: admin
  password: changeme123!
  email: admin@pllm.io

logging:
  level: debug

# Router configuration with adaptive routing for high-load scenarios
router:
  routing_strategy: "latency-based"
  circuit_breaker_enabled: true
  circuit_breaker_threshold: 5
  circuit_breaker_cooldown: 30s

  # Fallback chains - automatic failover when primary models are slow or failing
  fallbacks:
    my-gpt-4: ["my-gpt-35-turbo"]
    my-gpt-35-turbo: ["my-gpt-35-turbo-16k"]

# Model list - Users call these model names in API requests
model_list:
  # Primary GPT-4 instance
  - model_name: my-gpt-4
    params:
      model: gpt-4
      api_key: ${OPENAI_API_KEY}
    rpm: 60
    tpm: 90000

  # Primary GPT-3.5-Turbo instance
  - model_name: my-gpt-35-turbo
    params:
      model: gpt-3.5-turbo
      api_key: ${OPENAI_API_KEY}
    rpm: 200
    tpm: 90000

  # GPT-3.5-Turbo-16K as fallback
  - model_name: my-gpt-35-turbo-16k
    params:
      model: gpt-3.5-turbo-16k
      api_key: ${OPENAI_API_KEY}
    rpm: 200
    tpm: 180000

  # OpenRouter models - uses full model path as OpenRouter expects
  - model_name: deepseek-v3-free
    params:
      model: deepseek/deepseek-chat-v3-0324:free # OpenRouter format: provider/model
      api_key: ${OPENROUTER_API_KEY}
      api_base: https://openrouter.ai/api/v1
    rpm: 60
    tpm: 50000
