# PLLM Docker Configuration

server:
  port: 8080
  admin_port: 9090
  metrics_port: 8081

cors:
  allowed_origins: ["http://localhost:3000", "http://localhost:5173", "*"]
  allowed_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
  allowed_headers: ["Content-Type", "Authorization", "X-API-Key"]
  exposed_headers: ["X-Request-ID"]
  allow_credentials: true
  max_age: 3600

database:
  url: postgres://pllm:pllm@postgres:5432/pllm?sslmode=disable

redis:
  url: redis://redis:6379

jwt:
  secret_key: your-super-secret-jwt-key-change-this

admin:
  username: admin
  password: changeme123!
  email: admin@pllm.io

logging:
  level: debug

# Authentication configuration
auth:
  master_key: sk-master-dev-key-change-in-production
  require_auth: true
  dex:
    enabled: true
    # Backend OIDC validation (internal service URL)
    issuer: "http://dex:5556/dex"
    # Frontend OAuth flows (public URL for browser)
    public_issuer: "http://localhost:5556/dex"
    client_id: "pllm-web"
    client_secret: "pllm-web-secret"
    redirect_url: "http://localhost:3000/auth/callback"
    scopes: ["openid", "profile", "email", "groups"]
    # Enabled OAuth providers (must match connector IDs in your dex config)
    # Remove providers you don't want to show in the login page
    enabled_providers: ["github", "microsoft"] # Removed "google"

# Router configuration with adaptive routing for high-load scenarios
router:
  routing_strategy: "latency-based"
  circuit_breaker_enabled: true
  circuit_breaker_threshold: 5
  circuit_breaker_cooldown: 30s

  # Fallback chains - automatic failover when primary models are slow or failing
  fallbacks:
    my-gpt-4: ["my-gpt-35-turbo"]
    my-gpt-35-turbo: ["my-gpt-35-turbo-16k"]

# Model list - Users call these model names in API requests
model_list:
  # Primary OpenAI GPT-4 instance with custom pricing
  - model_name: my-gpt-4
    provider:
      type: openai
      model: gpt-4
      api_key: ${OPENAI_API_KEY}
    model_info:
      mode: chat
      supports_functions: true
      supports_vision: false
      max_tokens: 8192
      max_input_tokens: 8192
      max_output_tokens: 4096
    # Custom pricing override for production environment
    input_cost_per_token: 0.00003   # Standard GPT-4 rate
    output_cost_per_token: 0.00006  # Standard GPT-4 rate
    rpm: 60
    tpm: 90000
    priority: 100
    enabled: true
    tags: ["production", "openai", "gpt-4"]

  # GPT-4o with updated pricing
  - model_name: gpt-4o
    provider:
      type: openai
      model: gpt-4o
      api_key: ${OPENAI_API_KEY}
    model_info:
      mode: chat
      supports_functions: true
      supports_vision: true
      supports_audio_input: true
      max_tokens: 128000
      max_input_tokens: 128000
      max_output_tokens: 16384
    # GPT-4o pricing (more cost-effective than GPT-4)
    input_cost_per_token: 0.0000025  # Lower than GPT-4
    output_cost_per_token: 0.00001   # Lower than GPT-4
    rpm: 60
    tpm: 90000
    priority: 95
    enabled: true
    tags: ["production", "openai", "gpt-4o", "multimodal"]

  # Primary GPT-3.5-Turbo instance
  - model_name: my-gpt-35-turbo
    provider:
      type: openai
      model: gpt-3.5-turbo
      api_key: ${OPENAI_API_KEY}
    model_info:
      mode: chat
      supports_functions: true
      supports_vision: false
      max_tokens: 16385
      max_input_tokens: 16385
      max_output_tokens: 4096
    # GPT-3.5-Turbo pricing (budget-friendly)
    input_cost_per_token: 0.0000015  # Very cost-effective
    output_cost_per_token: 0.000002  # Very cost-effective
    rpm: 200
    tpm: 90000
    priority: 80
    enabled: true
    tags: ["production", "openai", "gpt-3.5", "budget"]

  # GPT-3.5-Turbo-16K as fallback
  - model_name: my-gpt-35-turbo-16k
    provider:
      type: openai
      model: gpt-3.5-turbo-16k
      api_key: ${OPENAI_API_KEY}
    model_info:
      mode: chat
      supports_functions: true
      supports_vision: false
      max_tokens: 16385
      max_input_tokens: 16385
      max_output_tokens: 4096
    # Same as regular GPT-3.5-Turbo
    input_cost_per_token: 0.0000015
    output_cost_per_token: 0.000002
    rpm: 200
    tpm: 180000
    priority: 75
    enabled: true
    tags: ["fallback", "openai", "gpt-3.5", "budget"]

  # OpenRouter DeepSeek V3 (Free tier)
  - model_name: deepseek-v3-free
    provider:
      type: openai  # OpenRouter uses OpenAI-compatible API
      model: deepseek/deepseek-chat-v3-0324:free
      api_key: ${OPENROUTER_API_KEY}
      base_url: https://openrouter.ai/api/v1
    model_info:
      mode: chat
      supports_functions: true
      supports_vision: false
      max_tokens: 64000
      max_input_tokens: 64000
      max_output_tokens: 8192
    # Free tier - no cost but rate limited
    input_cost_per_token: 0.0   # Free tier
    output_cost_per_token: 0.0  # Free tier
    rpm: 60
    tpm: 50000
    priority: 60
    enabled: true
    tags: ["openrouter", "deepseek", "free", "experimental"]

  # OpenRouter Gemini 2.0 Flash (Free tier)
  - model_name: gemini-2.0-flash-exp-free
    provider:
      type: openai  # OpenRouter uses OpenAI-compatible API
      model: google/gemini-2.0-flash-exp:free
      api_key: ${OPENROUTER_API_KEY}
      base_url: https://openrouter.ai/api/v1
    model_info:
      mode: chat
      supports_functions: true
      supports_vision: true
      supports_audio_input: true
      max_tokens: 1048576  # 1M context window
      max_input_tokens: 1048576
      max_output_tokens: 8192
    # Free tier - no cost but rate limited
    input_cost_per_token: 0.0   # Free tier
    output_cost_per_token: 0.0  # Free tier
    rpm: 60
    tpm: 50000
    priority: 55
    enabled: true
    tags: ["openrouter", "google", "gemini", "free", "multimodal"]

  # OpenRouter GPT OSS 20B (Free tier)
  - model_name: gpt-oss-20b-free
    provider:
      type: openai  # OpenRouter uses OpenAI-compatible API
      model: openai/gpt-oss-20b:free
      api_key: ${OPENROUTER_API_KEY}
      base_url: https://openrouter.ai/api/v1
    model_info:
      mode: chat
      supports_functions: false
      supports_vision: false
      max_tokens: 8192
      max_input_tokens: 8192
      max_output_tokens: 2048
    # Free tier - no cost but rate limited
    input_cost_per_token: 0.0   # Free tier
    output_cost_per_token: 0.0  # Free tier
    rpm: 60
    tpm: 50000
    priority: 50
    enabled: true
    tags: ["openrouter", "oss", "free", "experimental"]

  # OpenRouter Claude 3 Haiku (Paid tier)
  - model_name: claude-3-haiku
    provider:
      type: openai  # OpenRouter uses OpenAI-compatible API
      model: anthropic/claude-3-haiku
      api_key: ${OPENROUTER_API_KEY}
      base_url: https://openrouter.ai/api/v1
    model_info:
      mode: chat
      supports_functions: false
      supports_vision: true
      max_tokens: 200000
      max_input_tokens: 200000
      max_output_tokens: 4096
    # Claude 3 Haiku pricing via OpenRouter
    input_cost_per_token: 0.00000025  # Very cost-effective
    output_cost_per_token: 0.00000125 # Very cost-effective
    rpm: 60
    tpm: 50000
    priority: 70
    enabled: true
    tags: ["openrouter", "anthropic", "claude", "budget", "multimodal"]
